{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raport - tweets\n",
    "### Piotr Konowalski 109727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "Poniższy raport przedstawia przebieg analizy zbioru Tweetów celem ich zaklasyfikowania jako \"pozytywne\", \"neutralne\" lub \"negatywne. Treninowy zbiór danych zawiera jedynie 5969 wpisów co przy analizie języka naturalnego jest dość skromną liczbą. Zbiór treningowy został poddany wstępnemu przetwarzaniu w celu sprowadzenia danych do postaci, która w łatwiejszy sposób poddaje się analizie. Ostatecznie do klasyfikacji użyto algorytmu Random Forrest. Pozwoliło to na uzyskanie dokładności klasyfikacji na poziomie ok. 56%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Do usunięcia stopwords wykorzystano listę dostępną w bibliotece nltk (from nltk.corpus import stopwords). W celu osiągnięcia większej elastyczności lista została pobrana ze źródła, tak aby łatwo można było edytować jej zawartość. Do listy dodano znaki interpunkcyjne oraz usunięto z niej wyrazy odpowiadające za tworzenie negacji, które mogą się okazać przydane w dalszej analizie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "             \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
    "             \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "             \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
    "             \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
    "             \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "             \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
    "             \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"with\", \"would\", \"you\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "             \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
    "             \"didn't\", \"he's\", \"she's\", \"i'll\", \"you'll\", \"she'll\", \"he'll\", \"we'll\", \"they'll\", \"you're\", \"we're\",\n",
    "             \"i'd\", \"you'd\", \"i'm\", \"i've\", \"you've\", \"we've\",\n",
    "             \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"off\",\n",
    "             \"out\", \"over\", \"same\", \"too\", \"under\", \"why\",\n",
    "             # \"no\", \"nor\", \"not\",\n",
    "             ]\n",
    "\n",
    "punctuationChars = [',', '.', ':', '?', '!', '*', '(', ')', '\"', \"'\", ';', '(', ')', '...', '-', '&', '^', '*', '%',\n",
    "                    '$', '#']\n",
    "stopwords += punctuationChars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja, stemming i lematyzacja\n",
    "\n",
    "Do tokenizacji został wykorzystany TweetTokenizer. W przeciwieństwie do standardowego tokenizatora pozwala on na bardziej elastyczne dostosowanie parametrów tokenizacji. Jest to dedykowane narzędzie do tokenizacji tweetów, dzięki czemu poprawnie tokenizuje hashtagi, nazwy użytkowników oraz emotikony.\n",
    "\n",
    "Do stemmingu wykorzystano standardowy dla języka angielskiego PorterStemmer. W celu otrzymania wersji bazowej słowa przed stemmingiem tokeny zostały poddane lematyzacji z wykorzystaniem WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lmt = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "tweets = pd.read_csv('train.csv', '.', ',', header=0)\n",
    "\n",
    "\n",
    "def prepare_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = list(filter(lambda t: t not in stopwords, tokens))\n",
    "    tokens = [ps.stem(lmt.lemmatize(t)) for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje pomocnicze\n",
    "Funkcja zliczająca słowa, pozwalająca na dalszym etapie wykluczyć słowa zbyt rzadko lub zbyt często pojawiające się:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words():\n",
    "    words = Counter()\n",
    "    for i in tweets.index:\n",
    "        tweet = tweets.loc[i, 'Tweet']\n",
    "        tokens = prepare_tokens(tweet)\n",
    "        words.update(tokens)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja tworząca strukturę Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bow(documents, features, read_labels=True):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in documents.index:\n",
    "        tweet = documents.loc[i, 'Tweet']\n",
    "        tweet_tokens = prepare_tokens(tweet)\n",
    "\n",
    "        if read_labels:\n",
    "            label = documents.loc[i, 'Category']\n",
    "            labels.append(label)\n",
    "\n",
    "        for token in set(tweet_tokens):\n",
    "            if token not in features:\n",
    "                continue\n",
    "            row.append(i)\n",
    "            col.append(features[token])\n",
    "            data.append(1)\n",
    "    return csr_matrix((data, (row, col)), shape=(len(documents), len(features))), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje służące do tesowania klasyfikatorów i wyświetlania wyników testów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_classifier(classifier, dataset, feature_dict, list_of_labels):\n",
    "    x_test, _ = create_bow(dataset, feature_dict)\n",
    "    predicted = classifier.predict(x_test)\n",
    "    expected = dataset.loc[:, 'Category']\n",
    "    print_results(expected, predicted, list_of_labels)\n",
    "\n",
    "\n",
    "def print_results(expected, predicted, list_of_labels):\n",
    "    print(\"=================== Results ===================\")\n",
    "    print(\"            Positive    Neutral     Negative   \")\n",
    "    print(\"F1       \", f1_score(expected, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
    "    print(\"Precision\", precision_score(expected, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
    "    print(\"Recall   \", recall_score(expected, predicted, average=None, pos_label=None, labels=list_of_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    common_words = get_common_words()\n",
    "    common_words = list([k for k, v in common_words.most_common() if v > 5])\n",
    "\n",
    "    feature_dict = {}\n",
    "    for word in common_words:\n",
    "        feature_dict[word] = len(feature_dict)\n",
    "\n",
    "    train, test = train_test_split(tweets, test_size=0.25, random_state=42)\n",
    "    columns = train.columns\n",
    "    train = pd.DataFrame(train.as_matrix())\n",
    "    train.columns = columns\n",
    "    test = pd.DataFrame(test.as_matrix())\n",
    "    test.columns = columns\n",
    "\n",
    "    print(\"Training classifier...\")\n",
    "\n",
    "    dataset = tweets\n",
    "    # dataset = train\n",
    "\n",
    "    x_train, y_train = create_bow(dataset, feature_dict)\n",
    "    list_of_labels = list(set(y_train))\n",
    "\n",
    "    forrest = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
    "    forrest.fit(x_train, y_train)\n",
    "\n",
    "    # ada = AdaBoostClassifier(n_estimators=300, )\n",
    "    # ada.fit(x_train, y_train)\n",
    "    #\n",
    "    # nb = MultinomialNB()\n",
    "    # nb.fit(x_train, y_train)\n",
    "    #\n",
    "    # knn = KNeighborsClassifier()\n",
    "    # knn.fit(x_train, y_train)\n",
    "\n",
    "    # classifiers = [forrest, ada, nb, knn]\n",
    "\n",
    "    classifiers = [forrest]\n",
    "\n",
    "    # print(\"Testing...\")\n",
    "\n",
    "    # for classifier in classifiers:\n",
    "    #     test_classifier(classifier, test, feature_dict, list_of_labels)\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    \n",
    "    test = pd.read_csv(\"test.csv\", '.', ',', header=0)\n",
    "    x_test, _ = create_bow(test, feature_dict, read_labels=False)\n",
    "    predicted = forrest.predict(x_test)\n",
    "    test['Category'] = pd.Series(predicted, index=test.index)\n",
    "\n",
    "    # for i in test.head().index:\n",
    "    #     tokens = prepare_tokens(test.loc[i, 'Tweet'])\n",
    "    #     if not set([':)', ':-)', ';)', ';-)']).isdisjoint(tokens):\n",
    "    #         test.loc[i, 'Category'] = 'positive'\n",
    "    #\n",
    "    #     if not set([':(', ':-(', ';(', ';-(']).isdisjoint(tokens):\n",
    "    #         test.loc[i, 'Category'] = 'negative'\n",
    "\n",
    "    result = test.loc[:, ['Id', 'Category']]\n",
    "    result.to_csv('result.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "W ramach wstępnego przetwarzania Tweety zostały stokenizowane i na tej podstawie utworzono zbiór najbardziej popularnych słów (common_words), z którego nastepnie usunięto słowa występujące rzadziej niż 5 razy w zbiorze treningowym. Podjęto również próbę usunięcia najbardziej popularnych słów, jednak takie podejście nie pozwoliło na poprawę wyniku klasyfikacji. Wynika to prawdopodobnie z faktu, że większość najbardziej popularnych słów została usunięta z listy już na etapie tokenizacji przy analizie stopwords. \n",
    "\n",
    "Na podstawie zbioru common_words utowrzono słownik feature_dict, który odwzorowuje każde słowo na liczbę. Taka reprezentacja zostaje następnie wykorzystana do utworzenia struktury Bag of Words, w której każdy dokument przyjmuje formę wektora liczb. \n",
    "\n",
    "\n",
    "### Uczenie \n",
    "\n",
    "Dostępny zbiór danych zostaje podzielony na zbiór testowy i treningowy w stosunku 3:1. Po serii przeprowadzonych testów okazało się, że najlepsze wyniki daje algorytm Random Forrest. Pozwoliło to na uzyskanie dokładności na poziomie ok. 56%. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
